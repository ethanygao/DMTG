_BASE_: base_taskonomy_100_epoches_5_tasks.yaml
train_dataset:
  name: ffaster_taskonomy
  args:
    data_dir: /remote-home/share/datasets/taskonomy_dataset/larger_pkl_dataset/
    model_whitelist: datasets/tiny_train_models.txt
    augment: True
  batch_size: 20

val_dataset:
  name: ffaster_taskonomy
  args:
    data_dir: /remote-home/share/datasets/taskonomy_dataset/larger_pkl_dataset/
    model_whitelist: datasets/tiny_val_models.txt
  batch_size: 20

meta_arch:
  name: learnToGroup
  args:
    k_groups: 3
    sample: gumbel
    regularize_grad: trevor_rep_function
    backbone:
      name: vit_encoder
      args:
        model_timm: vit_base_patch16_224
        img_size: 256
        patch_size: 16
        first_k_layers: 6
    group_bone:
      name: vit_group_encoder
      args:
        model_timm: vit_base_patch16_224
        img_size: 256
        patch_size: 16
        behind_first_k_layers: 6
    decoders:
      name: taskonomy_decoder
      args:
        channels_per_task: [1, 3, 18, 1, 1]

loss:
  name: naive
  args:
    rotate: True

optimizer:
  name: adam
  args:
    lr: 1.e-3

scheduler:
  name: reduce_on_plateau
  args:
    mode: min
    min_lr: 3.e-5
    factor: 5.e-1
    patience: 3
    threshold: 1.e-1
